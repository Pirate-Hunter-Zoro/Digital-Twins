#!/bin/bash
#SBATCH --partition=c3_short
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --mem=20GB
#SBATCH --time=10:00:00
#SBATCH --output=logs/llama_cpp_server-%J.stdout
#SBATCH --error=logs/llama_cpp_server-%J.stderr
#SBATCH --job-name=llama_cpp_medgemma_server
#SBATCH --mail-user=mferguson@laureateinstitute.org
#SBATCH --mail-type=BEGIN,END,FAIL

echo "Starting llama.cpp server job for MedGemma GGUF."
echo "Job ID: $SLURM_JOB_ID"
echo "Running on host: $(hostname)"
echo "Current directory: $(pwd)"
echo "---"

# Load CUDA and CMake modules (if needed for the environment where this SBatch job runs)
# It's good practice to ensure the environment is fully set up in the SBatch script.
module load CUDA/12.5.0 # Or the specific version you used for compilation
module load CMake # Or the specific version you used for compilation

# Path to your compiled llama.cpp directory
LLAMA_CPP_DIR="/home/librad.laureateinstitute.org/mferguson/llama.cpp"
# Path to your downloaded MedGemma GGUF model
MEDGEMMA_MODEL_PATH="/home/librad.laureateinstitute.org/mferguson/models/MedGemma-27B-text-it-GGUF/medgemma-27b-text-it-Q4_K_M.gguf"
# The port the server will listen on
SERVER_PORT="8001"

# Check if model exists
if [ ! -f "$MEDGEMMA_MODEL_PATH" ]; then
    echo "ERROR: MedGemma GGUF model not found at $MEDGEMMA_MODEL_PATH. Exiting."
    exit 1
fi

echo "Launching llama.cpp HTTP server..."
# Run the llama.cpp server in the background
# CRITICAL CHANGE: Use 'bin/llama-server' instead of 'bin/server'
"$LLAMA_CPP_DIR"/build/bin/llama-server \
  --model "$MEDGEMMA_MODEL_PATH" \
  --port "$SERVER_PORT" \
  --n-gpu-layers -1 \
  --n-ctx 4096 \
  --host 0.0.0.0 > "$LLAMA_CPP_DIR"/server_output.log 2>&1 & # Redirect output to a log file

# Capture the PID of the background process
SERVER_PID=$!
echo "llama.cpp server launched with PID: $SERVER_PID"
echo "Server output is being redirected to: $LLAMA_CPP_DIR/server_output.log"

# Keep the Slurm job alive while the server runs in the background
# The server will stop when this job exits (due to time limit or scancel).
sleep infinity # This will keep the Slurm job active indefinitely (until time limit)

echo "llama.cpp server job finished (this message will only appear if 'sleep infinity' is interrupted)."