#!/bin/bash

#SBATCH --job-name=LIBR_Combined_Pipeline # A descriptive name for your single combined job
#SBATCH --partition=c3_short            
#SBATCH --ntasks=1                      # Request 1 task for this combined job script
#SBATCH --cpus-per-task=64              # Total CPU cores for both vLLM AND main.py. Node has 96.
#SBATCH --mem=160G                      # Sum of vLLM MEM (32G) + main.py MEM (128G). Compute nodes have 1TB, so this is fine.
#SBATCH --time=8:59:59                 # Total max wall-time for both parts (e.g., 24 hours). Adjust as needed.
#SBATCH --gres=gpu:1                    # REQUEST ONE GPU (for vLLM server part)
#SBATCH --output=logs/combined_pipeline_%j.stdout # Standard output log
#SBATCH --error=logs/combined_pipeline_%j.stderr   # Standard error log

# --- INSCRIBE THE RITUAL PARAMETERS HERE ---
NUM_PATIENTS=50
NUM_VISITS=6
VECTORIZER="sentence_transformer" 
DISTANCE_METRIC="euclidean"          
NUM_NEIGHBORS=10
NUM_WORKERS=60

echo "--- Ritual Parameters ---"
echo "Patients: $NUM_PATIENTS"
echo "Visits: $NUM_VISITS"
echo "Vectorizer: $VECTORIZER"
echo "Distance Metric: $DISTANCE_METRIC"
echo "Neighbors: $NUM_NEIGHBORS"
echo "Workers: $NUM_WORKERS"
echo "-------------------------"


# --- Environment Setup ---
# Load your Python module (verify exact name with 'module avail Python')
module load Python/3.11.5-GCCcore-13.2.0

# --- Activate Conda Environment ---
source /opt/apps/easybuild/software/Anaconda3/2022.05/etc/profile.d/conda.sh
conda activate vllm_env

# --- Navigate to your project directory ---
cd /mnt/dell_storage/homefolders/librad.laureateinstitute.org/mferguson/Digital-Twins/

# --- Start vLLM server in background ---
echo "$(date): Starting vLLM server in background..."
vllm serve unsloth/medgemma-27b-text-it-bnb-4bit \
  --dtype float16 \
  --served-model-name medgemma \
  --gpu-memory-utilization 0.5 \
  --host 0.0.0.0 \
  --max-model-len 5000 > logs/vllm_server.log 2>&1 & 
VLLM_PID=$!
echo "$(date): vLLM server started with PID: $VLLM_PID"

# --- Define cleanup function to kill vLLM server on script exit ---
cleanup() {
  echo "$(date): Cleaning up vLLM server (PID: $VLLM_PID)..."
  kill -TERM $VLLM_PID
  wait $VLLM_PID 2>/dev/null
  echo "$(date): vLLM server stopped and log cleaned."
}

# Trap signals to ensure cleanup function runs on script exit
trap cleanup EXIT

# --- Wait for vLLM server to be ready ---
echo "$(date): Waiting for vLLM server to be ready..."
MAX_WAIT_TIME=1200
WAIT_INTERVAL=5
SERVER_READY=0
for ((i=0; i<MAX_WAIT_TIME/WAIT_INTERVAL; i++)); do
  if curl -s -f http://localhost:8000/health >/dev/null; then
    echo "$(date): vLLM server is ready!"
    SERVER_READY=1
    break
  fi
  echo "$(date): vLLM server not yet ready, waiting..."
  sleep $WAIT_INTERVAL
done

if [ $SERVER_READY -eq 0 ]; then
  echo "$(date): ERROR: vLLM server did not become ready within the time limit. Exiting."
  exit 1
fi

# --- PHASE 1: Run Main Python Pipeline ---
echo "$(date): Starting main Python pipeline..."
# We now pass our parameters to the main script
$CONDA_PREFIX/bin/python scripts/main.py \
  --num_patients $NUM_PATIENTS \
  --num_visits $NUM_VISITS \
  --vectorizer_method $VECTORIZER \
  --distance_metric $DISTANCE_METRIC \
  --num_neighbors $NUM_NEIGHBORS \
  --workers $NUM_WORKERS 

echo "$(date): Main Python pipeline finished."


# --- PHASE 2: Run Visualization Script ---
echo "$(date): Starting visualization script..."
# We use the EXACT SAME parameters to ensure we visualize the correct results file.
$CONDA_PREFIX/bin/python scripts/analyze_results/visualize_results.py \
  --num_patients $NUM_PATIENTS \
  --num_visits $NUM_VISITS \
  --vectorizer_method $VECTORIZER \
  --distance_metric $DISTANCE_METRIC \
  --num_neighbors $NUM_NEIGHBORS

echo "$(date): Visualization script finished."


# Cleanup will be handled automatically by the trap EXIT
conda deactivate
echo "$(date): Ritual complete."