#!/bin/bash

#SBATCH --job-name=LIBR_Combined_Pipeline # A descriptive name for your single combined job
#SBATCH --partition=c3_short                  # Use 'c3' if you need >9 hours, otherwise 'c3_short' (adjust --time accordingly)
#SBATCH --ntasks=1                      # Request 1 task for this combined job script
#SBATCH --cpus-per-task=64              # Total CPU cores for both vLLM AND main.py. Node has 96.
#SBATCH --mem=160G                      # Sum of vLLM MEM (32G) + main.py MEM (128G). Compute nodes have 1TB, so this is fine.
#SBATCH --time=9:00:00                 # Total max wall-time for both parts (e.g., 24 hours). Adjust as needed.
#SBATCH --gres=gpu:1                    # REQUEST ONE GPU (for vLLM server part)
#SBATCH --output=logs/combined_pipeline_%j.stdout # Standard output log
#SBATCH --error=logs/combined_pipeline_%j.stderr   # Standard error log

# --- Environment Setup ---
# Load your Python module (verify exact name with 'module avail Python')
module load Python/3.11.5-GCCcore-13.2.0

# --- Activate Conda Environment ---
source /opt/apps/easybuild/software/Anaconda3/2022.05/etc/profile.d/conda.sh
conda activate vllm_env

# --- Navigate to your project directory ---
cd /mnt/dell_storage/homefolders/librad.laureateinstitute.org/mferguson/Digital-Twins/

# --- Start vLLM server in background ---
echo "$(date): Starting vLLM server in background..."
# The 'vllm serve' command needs to be in a subshell or have its output redirected
# to prevent it from blocking the main script or interfering with Slurm's stdout.
# We redirect stdout/stderr to a temporary log file and run in background (&).
vllm serve unsloth/medgemma-27b-text-it-bnb-4bit \
  --dtype float16 \
  --served-model-name medgemma \
  --gpu-memory-utilization 0.5 \
  --host 0.0.0.0 \
  --max-model-len 5000 > logs/vllm_server.log 2>&1 & 
VLLM_PID=$! # Capture the Process ID of the background vLLM server
echo "$(date): vLLM server started with PID: $VLLM_PID"

# --- Define cleanup function to kill vLLM server on script exit ---
cleanup() {
  echo "$(date): Cleaning up vLLM server (PID: $VLLM_PID)..."
  # Use 'kill -TERM' for graceful shutdown, wait a bit, then 'kill -9' if needed
  kill -TERM $VLLM_PID
  wait $VLLM_PID 2>/dev/null # Wait for the background process to terminate
  echo "$(date): vLLM server stopped and log cleaned."
}

# Trap signals to ensure cleanup function runs on script exit (even if it crashes)
trap cleanup EXIT

# --- Wait for vLLM server to be ready ---
echo "$(date): Waiting for vLLM server to be ready..."
MAX_WAIT_TIME=1200 # Max 20 minutes (1200 seconds) - Increased for large model startup
WAIT_INTERVAL=5
SERVER_READY=0
for ((i=0; i<MAX_WAIT_TIME/WAIT_INTERVAL; i++)); do
  # NEW: Correct the health check URL to /health
  if curl -s -f http://localhost:8000/health >/dev/null; then
    echo "$(date): vLLM server is ready!"
    SERVER_READY=1
    break
  fi
  echo "$(date): vLLM server not yet ready, waiting..."
  sleep $WAIT_INTERVAL
done

if [ $SERVER_READY -eq 0 ]; then
  echo "$(date): ERROR: vLLM server did not become ready within the time limit. Exiting."
  exit 1
fi

# --- Run main Python script ---
echo "$(date): Starting main Python pipeline..."
# NEW: Explicitly call the Python executable from the activated environment
$CONDA_PREFIX/bin/python scripts/main.py

echo "$(date): Main Python pipeline finished."

# Cleanup will be handled by the trap EXIT
conda deactivate # Still good practice, though trap handles most exits