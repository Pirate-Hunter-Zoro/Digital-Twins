#!/bin/bash
#SBATCH --partition=c3_short
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=80G
#SBATCH --gres=gpu:1
#SBATCH --time=9:00:00

# === Dynamic Config Parameters ===
REP=$1
VEC=$2
MODEL_NAME=$3

# === Log Handling ===
JOB_NAME="debug_${REP}_${VEC}_${MODEL_NAME}"
OUT_LOG="logs/debug_prompt_test_${JOB_NAME}_out.txt"
ERR_LOG="logs/debug_prompt_test_${JOB_NAME}_err.txt"

echo "ðŸ“› Job Name: $JOB_NAME"
echo "ðŸ“¤ Output Log: $OUT_LOG"
echo "ðŸ“¥ Error Log: $ERR_LOG"

# === Environment Setup ===
module load Python/3.11.5-GCCcore-13.2.0
source /opt/apps/easybuild/software/Anaconda3/2022.05/etc/profile.d/conda.sh
conda activate vllm_env

cd /mnt/dell_storage/homefolders/librad.laureateinstitute.org/mferguson/Digital-Twins

# === ðŸš€ Launch vLLM Server ===
echo "$(date): Starting vLLM server for model: $MODEL_NAME" | tee -a "$OUT_LOG"
vllm serve "unsloth/${MODEL_NAME}-27b-text-it-bnb-4bit" \
  --dtype float16 \
  --served-model-name "$MODEL_NAME" \
  --gpu-memory-utilization 0.5 \
  --host 0.0.0.0 \
  --max-model-len 5000 >> "$OUT_LOG" 2>> "$ERR_LOG" &

VLLM_PID=$!
trap "kill $VLLM_PID" EXIT

# Wait for vLLM server to be ready
for i in {1..120}; do
  if curl -s http://localhost:8000/health | grep -q "OK"; then
    echo "$(date): vLLM server is healthy!" | tee -a "$OUT_LOG"
    break
  fi
  sleep 2
done

# === ðŸ§  Run Prompt Debug ===
echo "$(date): Running prompt test..." | tee -a "$OUT_LOG"
$CONDA_PREFIX/bin/python scripts/debug/debug_prompt_test.py \
  --representation_method "$REP" \
  --vectorizer_method "$VEC" \
  --distance_metric "euclidean" \
  --num_visits 6 \
  --num_patients 5000 \
  --num_neighbors 5 >> "$OUT_LOG" 2>> "$ERR_LOG"

echo "$(date): âœ… Debug prompt test complete!" | tee -a "$OUT_LOG"
