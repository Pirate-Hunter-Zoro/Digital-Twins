#!/bin/bash
#SBATCH --partition=c3_short
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=80G
#SBATCH --gres=gpu:1
#SBATCH --time=9:00:00

# === ðŸ“¥ Accept Parameters ===
REP=$1
VEC=$2
DIST=$3
NEIGHBORS=$4
VISITS=$5
PATIENTS=$6
MODEL_NAME=$7
MODEL_PATH=$8

# === ðŸ·ï¸ Logging Setup ===
JOB_NAME="predict_${REP}_${VEC}_${DIST}_${MODEL_NAME}"
OUT_LOG="logs/${JOB_NAME}_out.txt"
ERR_LOG="logs/${JOB_NAME}_err.txt"

echo "ðŸ“› Job Name: $JOB_NAME"
echo "ðŸ“¤ Output Log: $OUT_LOG"
echo "ðŸ“¥ Error Log: $ERR_LOG"

# === âš™ï¸ Environment Setup ===
module load Python/3.11.5-GCCcore-13.2.0
source /opt/apps/easybuild/software/Anaconda3/2022.05/etc/profile.d/conda.sh
conda activate dt_env

cd /mnt/dell_storage/homefolders/librad.laureateinstitute.org/mferguson/Digital-Twins

# === ðŸš€ Launch vLLM Server ===
echo "$(date): Starting vLLM server for model: $MODEL_NAME" | tee -a "$OUT_LOG"
vllm serve $MODEL_PATH \
  --dtype float16 \
  --served-model-name "$MODEL_NAME" \
  --gpu-memory-utilization 0.5 \
  --host 0.0.0.0 \
  --max-model-len 5000 > "logs/vllm_server_${MODEL_NAME}.log" 2>&1 &

VLLM_PID=$!
trap "kill $VLLM_PID" EXIT

# Wait for server to be healthy
for i in {1..120}; do
  if curl -s http://localhost:8000/health | grep -q "OK"; then
    echo "$(date): âœ… vLLM is healthy!" | tee -a "$OUT_LOG"
    break
  fi
  sleep 2
done

# === ðŸ§  Run Prediction Script ===
echo "$(date): Running generate_patient_predictions.py..." | tee -a "$OUT_LOG"

$CONDA_PREFIX/bin/python scripts/llm/generate_patient_predictions.py \
  --representation_method "$REP" \
  --vectorizer_method "$VEC" \
  --distance_metric "$DIST" \
  --num_neighbors "$NEIGHBORS" \
  --num_visits "$VISITS" \
  --num_patients "$PATIENTS" \
  --model_name "$MODEL_NAME" >> "$OUT_LOG" 2>> "$ERR_LOG"

echo "$(date): ðŸ§ª Prediction finished for $REP | $VEC | $DIST | $MODEL_NAME" | tee -a "$OUT_LOG"
