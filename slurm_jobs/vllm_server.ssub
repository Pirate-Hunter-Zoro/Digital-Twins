#!/bin/bash

#SBATCH --job-name=vLLM_MedGemma_Server # A descriptive name for your LLM server job
#SBATCH --partition=c3                  # Specify the partition (e.g., c3 for general use)
#SBATCH --ntasks=1                      # Request 1 task (the server itself)
#SBATCH --cpus-per-task=8               # vLLM can use some CPU cores; adjust based on model size/usage
#SBATCH --mem=32G                       # Allocate enough RAM for vLLM (model, KV cache, etc.)
#SBATCH --time=24:00:00                 # Set a long wall-time for the server (e.g., 24 hours)
#SBATCH --gres=gpu:1                    # REQUEST ONE GPU for the vLLM server
#SBATCH --output=logs/vllm_server_%j.stdout # Standard output log
#SBATCH --error=logs/vllm_server_%j.stderr   # Standard error log
#SBATCH --nodelist=compute300

# --- Environment Setup ---
module load Python/3.11.5-GCCcore-13.2.0 # Use the available Python module
# NEW: Use the precise, absolute path you found for your environment activation
source /home/librad.laureateinstitute.org/mferguson/.conda/envs/vllm_env/bin/activate

# --- Navigate to your project directory (optional, but good for context) ---
cd /mnt/dell_storage/homefolders/librad.laureateinstitute.org/mferguson/Digital-Twins/

# --- Start the vLLM server ---
echo "Starting vLLM server..."
vllm serve unsloth/medgemma-27b-text-it-bnb-4bit \
  --dtype float16 \
  --served-model-name medgemma \
  --gpu-memory-utilization 0.5 \
  --host 0.0.0.0 \
  --max-model-len 5000 

echo "vLLM server started."

conda deactivate